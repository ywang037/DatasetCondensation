{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "from unittest import TestLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
    "from utils import data_preparation, gen_data_partition_iid, gen_data_partition_dirichlet, make_client_dataset_from_partition\n",
    "\n",
    "import random\n",
    "from client import ClientDC\n",
    "from server import ServerDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argparser():\n",
    "    parser = argparse.ArgumentParser(description='Federated DC using gradient matching')\n",
    "\n",
    "    # default args\n",
    "    parser.add_argument('--method', type=str, default='DC', help='DC/DSA')\n",
    "    parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "    parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "    parser.add_argument('--ipc', type=int, default=1, help='image(s) per class')\n",
    "    parser.add_argument('--eval_mode', type=str, default='S', help='eval_mode') # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
    "    parser.add_argument('--num_exp', type=int, default=5, help='the number of experiments')\n",
    "    parser.add_argument('--num_eval', type=int, default=20, help='the number of evaluating randomly initialized models')\n",
    "    parser.add_argument('--epoch_eval_train', type=int, default=300, help='epochs to train a model with synthetic data')\n",
    "    parser.add_argument('--Iteration', type=int, default=1000, help='training iterations')\n",
    "    parser.add_argument('--lr_img', type=float, default=0.1, help='learning rate for updating synthetic images')\n",
    "    parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "    parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "    parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "    parser.add_argument('--init', type=str, default='noise', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "    parser.add_argument('--dsa_strategy', type=str, default='None', help='differentiable Siamese augmentation strategy')\n",
    "    parser.add_argument('--data_path', type=str, default='data', help='dataset path')\n",
    "    parser.add_argument('--save_path', type=str, default='result', help='path to save results')\n",
    "    parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "\n",
    "    # addon args\n",
    "    parser.add_argument('--num_clients', type=int, default=5, help='number of clients')\n",
    "    parser.add_argument('--seed', type=int, default=3, help='set a seed for reproducability, set to 0 to activate randomness')\n",
    "    parser.add_argument('--client_alpha', type=float, default=100.0, help='dirichlet alpha for intra-cluster non-iid degree')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.outer_loop, args.inner_loop = get_loops(args.ipc)\n",
    "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "args=argparser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(method='DC', dataset='CIFAR10', model='ConvNet', ipc=1, eval_mode='S', num_exp=1, num_eval=5, epoch_eval_train=50, Iteration=100, lr_img=0.1, lr_net=0.01, batch_real=256, batch_train=256, init='noise', dsa_strategy='None', data_path='data', save_path='result', dis_metric='ours', num_clients=5, seed=3, client_alpha=100.0, outer_loop=10, inner_loop=10, device='cuda')\n"
     ]
    }
   ],
   "source": [
    "args.num_exp = 1\n",
    "args.num_eval = 5\n",
    "args.epoch_eval_train = 50\n",
    "args.Iteration=100\n",
    "args.outer_loop, args.inner_loop = 10, 10\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_it_pool:  [100]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "''' set seeds '''\n",
    "if args.seed:\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "rng = np.random.default_rng(args.seed)    \n",
    "\n",
    "''' some global setup '''\n",
    "# if not os.path.exists(args.data_path):\n",
    "#     os.mkdir(args.data_path)\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "\n",
    "# The list of iterations when we evaluate models and record results.\n",
    "# the defualt setting is to evaluate every 500 iterations, i.e., k=n*500\n",
    "# eval_it_pool = np.arange(0, args.Iteration+1, 500).tolist() if args.eval_mode == 'S' or args.eval_mode == 'SS' else [args.Iteration] \n",
    "eval_it_pool = [args.Iteration] # only evaluate at last iteration\n",
    "print('eval_it_pool: ', eval_it_pool)\n",
    "model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "data_set, data_info, testloader_server = data_preparation(args.dataset)\n",
    "\n",
    "accs_all_clients_all_exps = [dict() for i in range(args.num_clients)]\n",
    "for i in range(args.num_clients):\n",
    "    for key in model_eval_pool:\n",
    "        accs_all_clients_all_exps[i][key]=[]\n",
    "data_save_all_clients = [[] for i in range(args.num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== Exp 0 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 5, 'epoch_eval_train': 50, 'Iteration': 100, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'num_clients': 5, 'seed': 3, 'client_alpha': 100.0, 'outer_loop': 10, 'inner_loop': 10, 'device': 'cuda'}\n",
      "Evaluation model pool:  ['ConvNet']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "optimizer can only optimize Tensors, but one of the params is NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YWANG\\atr-data-distillation\\DatasetCondensation\\debug_FedDC.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m net_train \u001b[39m=\u001b[39m get_network(args\u001b[39m.\u001b[39mmodel, data_info[\u001b[39m'\u001b[39m\u001b[39mchannel\u001b[39m\u001b[39m'\u001b[39m], data_info[\u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m], data_info[\u001b[39m'\u001b[39m\u001b[39mimg_size\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m'''create clients and server'''\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m clients \u001b[39m=\u001b[39m [ClientDC(\u001b[39mid\u001b[39m, args, net_train, data_info, client_data_train[i], client_data_test[i], eval_it_pool, model_eval_pool) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mnum_clients)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m client \u001b[39min\u001b[39;00m clients:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mClient \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m training samples \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m testing samples\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(client\u001b[39m.\u001b[39mid, client\u001b[39m.\u001b[39mn_data_train, client\u001b[39m.\u001b[39mn_data_test))\n",
      "\u001b[1;32mc:\\Users\\YWANG\\atr-data-distillation\\DatasetCondensation\\debug_FedDC.ipynb Cell 6\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m net_train \u001b[39m=\u001b[39m get_network(args\u001b[39m.\u001b[39mmodel, data_info[\u001b[39m'\u001b[39m\u001b[39mchannel\u001b[39m\u001b[39m'\u001b[39m], data_info[\u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m], data_info[\u001b[39m'\u001b[39m\u001b[39mimg_size\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m'''create clients and server'''\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m clients \u001b[39m=\u001b[39m [ClientDC(\u001b[39mid\u001b[39;49m, args, net_train, data_info, client_data_train[i], client_data_test[i], eval_it_pool, model_eval_pool) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mnum_clients)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m client \u001b[39min\u001b[39;00m clients:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YWANG/atr-data-distillation/DatasetCondensation/debug_FedDC.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mClient \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m training samples \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m testing samples\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(client\u001b[39m.\u001b[39mid, client\u001b[39m.\u001b[39mn_data_train, client\u001b[39m.\u001b[39mn_data_test))\n",
      "File \u001b[1;32mc:\\Users\\YWANG\\atr-data-distillation\\DatasetCondensation\\client.py:50\u001b[0m, in \u001b[0;36mClientDC.__init__\u001b[1;34m(self, id, args, net_train, data_info, data_train, data_test, eval_it_pool, model_eval_pool)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_syn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mSGD([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_syn, ], lr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mlr_img, momentum\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m) \u001b[39m# optimizer for synthetic data\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m# optimizer_img.zero_grad()\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccs_all_exps \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\atr\\lib\\site-packages\\torch\\optim\\sgd.py:101\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m nesterov \u001b[39mand\u001b[39;00m (momentum \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m dampening \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    100\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m \u001b[39msuper\u001b[39;49m(SGD, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\atr\\lib\\site-packages\\torch\\optim\\optimizer.py:54\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     51\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: param_groups}]\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m param_group \u001b[39min\u001b[39;00m param_groups:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_param_group(param_group)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\atr\\lib\\site-packages\\torch\\optim\\optimizer.py:266\u001b[0m, in \u001b[0;36mOptimizer.add_param_group\u001b[1;34m(self, param_group)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m param_group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(param, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39moptimizer can only optimize Tensors, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mbut one of the params is \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtypename(param))\n\u001b[0;32m    268\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m param\u001b[39m.\u001b[39mis_leaf:\n\u001b[0;32m    269\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt optimize a non-leaf Tensor\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: optimizer can only optimize Tensors, but one of the params is NoneType"
     ]
    }
   ],
   "source": [
    "''' looping over multiple experiment trials '''\n",
    "for exp in range(args.num_exp):\n",
    "    print('\\n================== Exp %d ==================\\n '%exp)\n",
    "    print('Hyper-parameters: \\n', args.__dict__)\n",
    "    print('Evaluation model pool: ', model_eval_pool)\n",
    "\n",
    "    '''split data set for each client'''       \n",
    "    # generate training data partitioning using IID method\n",
    "    client_train_data_idcs, client_train_class_dict = gen_data_partition_iid(\n",
    "        data=data_set['train_data'], \n",
    "        num_classes=data_info['num_classes'], \n",
    "        labels=data_set['train_labels'], \n",
    "        data_mapp=data_set['mapp'],\n",
    "        num_clients=args.num_clients, \n",
    "        generator=rng, \n",
    "        verbose_hist=False)\n",
    "\n",
    "    client_test_data_idcs, client_test_class_dict = gen_data_partition_iid(\n",
    "        data=data_set['test_data'], \n",
    "        num_classes=data_info['num_classes'], \n",
    "        labels=data_set['test_labels'], \n",
    "        data_mapp=data_set['mapp'],\n",
    "        num_clients=args.num_clients, \n",
    "        generator=rng, \n",
    "        verbose_hist=False)\n",
    "\n",
    "    # make client data using the generated partition\n",
    "    client_data_train = make_client_dataset_from_partition(data_set['train_data'], args.num_clients, client_train_data_idcs)\n",
    "    client_data_test = make_client_dataset_from_partition(data_set['test_data'], args.num_clients, client_test_data_idcs)\n",
    "\n",
    "    '''set the architecture for the network to be trained'''\n",
    "    net_train = get_network(args.model, data_info['channel'], data_info['num_classes'], data_info['img_size']).to(args.device)\n",
    "\n",
    "    '''create clients and server'''\n",
    "    clients = [ClientDC(id, args, net_train, data_info, client_data_train[i], client_data_test[i], eval_it_pool, model_eval_pool) for i in range(args.num_clients)]\n",
    "    for client in clients:\n",
    "        print('Client {} has {} training samples {} testing samples'.format(client.id, client.n_data_train, client.n_data_test))\n",
    "    server = ServerDC(args, net_train, clients)\n",
    "    print('FL server created.')\n",
    "\n",
    "    ''' organize the real dataset and initialize the synthetic data '''\n",
    "    for client in clients:\n",
    "        client.organize_local_real_data()\n",
    "        for ch in range(client.channel):\n",
    "            print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(client.images_all[:, ch]), torch.std(client.images_all[:, ch])))\n",
    "        client.syn_data_init()\n",
    "        client.data_trainer_setup() # only to be called after syn_data_init()\n",
    "\n",
    "    ''' training starts from here '''\n",
    "    print('%s training begins'%get_time())\n",
    "    \n",
    "    # NOTE this loop is over the different model initializations, i.e., the loop indixed by K in the paper, Algorithm 1 line 4\n",
    "    for it in range(args.Iteration+1): \n",
    "        ''' get a new random initialization of the network '''\n",
    "        server.global_model = get_network(args.model, data_info['channel'], data_info['num_classes'], data_info['img_size']).to(args.device) \n",
    "        server.global_model_state = server.global_model.state_dict()\n",
    "        \n",
    "        for client in clients:\n",
    "            # ''' Evaluate synthetic data trained in last iteration'''\n",
    "            # client.syn_data_eval(exp, it)\n",
    "\n",
    "            # ''' get a new random initialization of the network '''\n",
    "            # client.model_train = get_network(args.model, client.channel, client.num_classes, client.im_size).to(args.device) \n",
    "\n",
    "            ''' fetch newly intialized server model weights '''\n",
    "            client.sync_with_server(server, method='state')\n",
    "\n",
    "            ''' set the optimizer for learning synthetic data '''\n",
    "            optimizer_net = client.net_trainer_setup(client.model_train)\n",
    "\n",
    "\n",
    "        # NOTE this loop is indixed by T in the paper, Algorithm 1 line 4\n",
    "        # this loop resembles the communication round in FL\n",
    "        for ol in range(args.outer_loop): \n",
    "            '''Server perform model aggregation upon local network updates'''\n",
    "            server.net_weights_aggregation(clients)\n",
    "\n",
    "            for client in clients:\n",
    "                ''' fetch server model weights '''\n",
    "                client.sync_with_server(server, method='state')\n",
    "\n",
    "                ''' freeze the running mu and sigma for BatchNorm layers '''\n",
    "                # Synthetic data batch, e.g. only 1 image/batch, is too small to obtain stable mu and sigma.\n",
    "                # So, we calculate and freeze mu and sigma for BatchNorm layer with real data batch ahead.\n",
    "                # This would make the training with BatchNorm layers easier.\n",
    "\n",
    "                BN_flag = False\n",
    "                BNSizePC = 16  # for batch normalization\n",
    "                for module in client.model_train.modules():\n",
    "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                        BN_flag = True\n",
    "                if BN_flag:\n",
    "                    img_real = torch.cat([client.get_images(c, BNSizePC) for c in range(client.num_classes)], dim=0)\n",
    "                    client.model_train.train() # for updating the mu, sigma of BatchNorm\n",
    "                    output_real = client.model_train(img_real) # get running mu, sigma\n",
    "                    for module in client.model_train.modules():\n",
    "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "                ''' update synthetic data '''\n",
    "                # one step of SGD, can be repeated for multiple steps\n",
    "                # update only once but over T iterations equivalent to T steps of SGD for learning the data\n",
    "                client.syn_data_update(client.model_train) \n",
    "\n",
    "                ''' update network '''\n",
    "                client.network_update(client.model_train, optimizer_net) \n",
    "                client.local_model_state = copy.deepcopy(client.model_train.state_dict()) # copy the updated local model weights to another iterables to avoid any unaware modification   \n",
    "\n",
    "        ''' Evaluate synthetic data trained in last iteration'''\n",
    "        for client in clients:\n",
    "            client.syn_data_eval(exp, it)\n",
    "            client.loss_avg /= (client.num_classes*args.outer_loop) # Summary for client data condensation for this exp trial\n",
    "            \n",
    "            if it%10 == 0:\n",
    "                print('%s iter = %04d, loss = %.4f' % (get_time(), it, client.loss_avg))\n",
    "            if it == args.Iteration: # only record the final results\n",
    "                data_save_all_clients[client.id].append([copy.deepcopy(client.image_syn.detach().cpu()), copy.deepcopy(client.label_syn.detach().cpu())])\n",
    "                torch.save({'data': data_save_all_clients[client.id], 'accs_all_exps': accs_all_clients_all_exps[client.id], }, os.path.join(client.save_path, 'res_%s_%s_%s_%dipc.pt'%(client.args.method, client.args.dataset, client.args.model, client.args.ipc)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n==================== Final Results ====================\\n')\n",
    "for i in range(args.num_clients):\n",
    "    for key in model_eval_pool:\n",
    "        accs = accs_all_clients_all_exps[i][key]\n",
    "        print('Client %d run %d experiments, train on %s, evaluate %d random %s, mean  = %.2f%%  std = %.2f%%'%(i, args.num_exp, args.model, len(accs), key, np.mean(accs)*100, np.std(accs)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('atr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fb7ec950fe6b28ba376f5ac2aa4547481897296ca80154442a226616f8f80b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
