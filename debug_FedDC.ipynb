{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "from unittest import TestLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
    "from utils import data_preparation, gen_data_partition_iid, gen_data_partition_dirichlet, make_client_dataset_from_partition\n",
    "\n",
    "import random\n",
    "from client import ClientDC\n",
    "from server import ServerDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argparser():\n",
    "    parser = argparse.ArgumentParser(description='Federated DC using gradient matching')\n",
    "\n",
    "    # default args\n",
    "    parser.add_argument('--method', type=str, default='DC', help='DC/DSA')\n",
    "    parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "    parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "    parser.add_argument('--ipc', type=int, default=1, help='image(s) per class')\n",
    "    parser.add_argument('--eval_mode', type=str, default='S', help='eval_mode') # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
    "    parser.add_argument('--num_exp', type=int, default=5, help='the number of experiments')\n",
    "    parser.add_argument('--num_eval', type=int, default=20, help='the number of evaluating randomly initialized models')\n",
    "    parser.add_argument('--epoch_eval_train', type=int, default=300, help='epochs to train a model with synthetic data')\n",
    "    parser.add_argument('--Iteration', type=int, default=1000, help='training iterations')\n",
    "    parser.add_argument('--lr_img', type=float, default=0.1, help='learning rate for updating synthetic images')\n",
    "    parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "    parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "    parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "    parser.add_argument('--init', type=str, default='noise', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "    parser.add_argument('--dsa_strategy', type=str, default='None', help='differentiable Siamese augmentation strategy')\n",
    "    parser.add_argument('--data_path', type=str, default='data', help='dataset path')\n",
    "    parser.add_argument('--save_path', type=str, default='result', help='path to save results')\n",
    "    parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "\n",
    "    # addon args\n",
    "    parser.add_argument('--num_clients', type=int, default=5, help='number of clients')\n",
    "    parser.add_argument('--seed', type=int, default=3, help='set a seed for reproducability, set to 0 to activate randomness')\n",
    "    parser.add_argument('--client_alpha', type=float, default=100.0, help='dirichlet alpha for intra-cluster non-iid degree')\n",
    "    parser.add_argument('--stand_alone', action='store_true', default=False, help='trigger non-federated local training mode')\n",
    "    parser.add_argument('--save_results', action='store_true', default=False, help='use this to save trained synthetic data and images')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.outer_loop, args.inner_loop = 10, 10\n",
    "    # args.outer_loop, args.inner_loop = get_loops(args.ipc)\n",
    "    args.dsa_param = ParamDiffAug()\n",
    "    args.dsa = False if args.dsa_strategy in ['none', 'None'] else True\n",
    "    \n",
    "    if args.stand_alone:\n",
    "        save_tag = args.dataset + '_local' + time.strftime('_%y-%m-%d-%H-%M-%S') \n",
    "    else:\n",
    "        save_tag = args.dataset + '_fed' + time.strftime('_%y-%m-%d-%H-%M-%S') \n",
    "\n",
    "    args.save_path = os.path.join(args.save_root, save_tag) \n",
    "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "args=argparser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'MNIST'\n",
    "args.num_exp = 1\n",
    "args.num_eval = 1\n",
    "args.epoch_eval_train = 1\n",
    "args.Iteration=10\n",
    "args.outer_loop, args.inner_loop = 10, 10\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' set seeds '''\n",
    "if args.seed:\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "rng = np.random.default_rng(args.seed)    \n",
    "\n",
    "''' some global setup '''\n",
    "# if not os.path.exists(args.data_path):\n",
    "#     os.mkdir(args.data_path)\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "\n",
    "# The list of iterations when we evaluate models and record results.\n",
    "# the defualt setting is to evaluate every 500 iterations, i.e., k=n*500\n",
    "# eval_it_pool = np.arange(0, args.Iteration+1, 500).tolist() if args.eval_mode == 'S' or args.eval_mode == 'SS' else [args.Iteration] \n",
    "eval_it_pool = [args.Iteration] # only evaluate at last iteration\n",
    "print('eval_it_pool: ', eval_it_pool)\n",
    "model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "data_set, data_info, testloader_server = data_preparation(args.dataset)\n",
    "\n",
    "accs_all_clients_all_exps = [dict() for i in range(args.num_clients)]\n",
    "for i in range(args.num_clients):\n",
    "    for key in model_eval_pool:\n",
    "        accs_all_clients_all_exps[i][key]=[]\n",
    "data_save_all_clients = [[] for i in range(args.num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' looping over multiple experiment trials '''\n",
    "for exp in range(args.num_exp):\n",
    "    print('\\n================== Exp %d ==================\\n '%exp)\n",
    "    print('Hyper-parameters: \\n', args.__dict__)\n",
    "    print('Evaluation model pool: ', model_eval_pool)\n",
    "\n",
    "    '''split data set for each client'''       \n",
    "    # generate training data partitioning using IID method\n",
    "    client_train_data_idcs, client_train_class_dict = gen_data_partition_iid(\n",
    "        data=data_set['train_data'], \n",
    "        num_classes=data_info['num_classes'], \n",
    "        labels=data_set['train_labels'], \n",
    "        data_mapp=data_set['mapp'],\n",
    "        num_clients=args.num_clients, \n",
    "        generator=rng, \n",
    "        verbose_hist=False)\n",
    "\n",
    "    client_test_data_idcs, client_test_class_dict = gen_data_partition_iid(\n",
    "        data=data_set['test_data'], \n",
    "        num_classes=data_info['num_classes'], \n",
    "        labels=data_set['test_labels'], \n",
    "        data_mapp=data_set['mapp'],\n",
    "        num_clients=args.num_clients, \n",
    "        generator=rng, \n",
    "        verbose_hist=False)\n",
    "\n",
    "    # make client data using the generated partition\n",
    "    client_data_train = make_client_dataset_from_partition(data_set['train_data'], args.num_clients, client_train_data_idcs)\n",
    "    client_data_test = make_client_dataset_from_partition(data_set['test_data'], args.num_clients, client_test_data_idcs)\n",
    "\n",
    "    '''set the architecture for the network to be trained'''\n",
    "    net_train = get_network(args.model, data_info['channel'], data_info['num_classes'], data_info['img_size']).to(args.device)\n",
    "\n",
    "    '''create clients and server'''\n",
    "    clients = [ClientDC(id, args, net_train, data_info, client_data_train[i], client_data_test[i], eval_it_pool, model_eval_pool) for i in range(args.num_clients)]\n",
    "    for client in clients:\n",
    "        print('Client {} has {} training samples {} testing samples'.format(client.id, client.num_local_data_train, client.num_local_data_test))\n",
    "    server = ServerDC(args, net_train, clients)\n",
    "    print('FL server created.')\n",
    "\n",
    "    ''' organize the real dataset and initialize the synthetic data '''\n",
    "    for client in clients:\n",
    "        client.organize_local_real_data()\n",
    "        for ch in range(client.channel):\n",
    "            print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(client.images_all[:, ch]), torch.std(client.images_all[:, ch])))\n",
    "        client.syn_data_init()\n",
    "        client.data_trainer_setup() # only to be called after syn_data_init()\n",
    "\n",
    "    ''' training starts from here '''\n",
    "    print('%s training begins'%get_time())\n",
    "    \n",
    "    # NOTE this loop is over the different model initializations, i.e., the loop indixed by K in the paper, Algorithm 1 line 4\n",
    "    for it in range(args.Iteration+1): \n",
    "        ''' get a new random initialization of the network '''\n",
    "        server.global_model = get_network(args.model, data_info['channel'], data_info['num_classes'], data_info['img_size']).to(args.device) \n",
    "        server.global_model_state = server.global_model.state_dict()\n",
    "        \n",
    "        for client in clients:\n",
    "            # ''' Evaluate synthetic data trained in last iteration'''\n",
    "            # client.syn_data_eval(exp, it)\n",
    "\n",
    "            # ''' get a new random initialization of the network '''\n",
    "            # client.model_train = get_network(args.model, client.channel, client.num_classes, client.im_size).to(args.device) \n",
    "\n",
    "            ''' fetch newly intialized server model weights '''\n",
    "            client.sync_with_server(server, method='state')\n",
    "\n",
    "            ''' set the optimizer for learning synthetic data '''\n",
    "            optimizer_net = client.net_trainer_setup(client.model_train)\n",
    "\n",
    "\n",
    "        # NOTE this loop is indixed by T in the paper, Algorithm 1 line 4\n",
    "        # this loop resembles the communication round in FL\n",
    "        for ol in range(args.outer_loop): \n",
    "            if not args.stand_alone:\n",
    "                '''Server perform model aggregation upon local network updates'''\n",
    "                server.net_weights_aggregation(clients)\n",
    "\n",
    "            for client in clients:\n",
    "                if not args.stand_alone:\n",
    "                    ''' fetch server model weights '''\n",
    "                    client.sync_with_server(server, method='state')\n",
    "\n",
    "                ''' freeze the running mu and sigma for BatchNorm layers '''\n",
    "                # Synthetic data batch, e.g. only 1 image/batch, is too small to obtain stable mu and sigma.\n",
    "                # So, we calculate and freeze mu and sigma for BatchNorm layer with real data batch ahead.\n",
    "                # This would make the training with BatchNorm layers easier.\n",
    "\n",
    "                BN_flag = False\n",
    "                BNSizePC = 16  # for batch normalization\n",
    "                for module in client.model_train.modules():\n",
    "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                        BN_flag = True\n",
    "                if BN_flag:\n",
    "                    img_real = torch.cat([client.get_images(c, BNSizePC) for c in range(client.num_classes)], dim=0)\n",
    "                    client.model_train.train() # for updating the mu, sigma of BatchNorm\n",
    "                    output_real = client.model_train(img_real) # get running mu, sigma\n",
    "                    for module in client.model_train.modules():\n",
    "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "                ''' update synthetic data '''\n",
    "                # one step of SGD, can be repeated for multiple steps\n",
    "                # update only once but over T iterations equivalent to T steps of SGD for learning the data\n",
    "                client.syn_data_update(client.model_train) \n",
    "\n",
    "                ''' update network '''\n",
    "                client.network_update(client.model_train, optimizer_net) \n",
    "                client.local_model_state = copy.deepcopy(client.model_train.state_dict()) # copy the updated local model weights to another iterables to avoid any unaware modification   \n",
    "\n",
    "        ''' Evaluate synthetic data trained in last iteration'''\n",
    "        for client in clients:\n",
    "            client.syn_data_eval(exp, it)\n",
    "            client.loss_avg /= (client.num_classes*args.outer_loop) # Summary for client data condensation for this exp trial\n",
    "            \n",
    "            if it%10 == 0:\n",
    "                print('%s iter = %04d, loss = %.4f' % (get_time(), it, client.loss_avg))\n",
    "            if it == args.Iteration: # only record the final results\n",
    "                data_save_all_clients[client.id].append([copy.deepcopy(client.image_syn.detach().cpu()), copy.deepcopy(client.label_syn.detach().cpu())])\n",
    "                torch.save({'data': data_save_all_clients[client.id], 'accs_all_exps': accs_all_clients_all_exps[client.id], }, os.path.join(client.save_path, 'res_%s_%s_%s_%dipc.pt'%(client.args.method, client.args.dataset, client.args.model, client.args.ipc)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n==================== Final Results ====================\\n')\n",
    "for i in range(args.num_clients):\n",
    "    for key in model_eval_pool:\n",
    "        accs = accs_all_clients_all_exps[i][key]\n",
    "        print('Client %d run %d experiments, train on %s, evaluate %d random %s, mean  = %.2f%%  std = %.2f%%'%(i, args.num_exp, args.model, len(accs), key, np.mean(accs)*100, np.std(accs)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('atr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fb7ec950fe6b28ba376f5ac2aa4547481897296ca80154442a226616f8f80b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
